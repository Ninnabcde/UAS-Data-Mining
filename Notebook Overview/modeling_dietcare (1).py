# -*- coding: utf-8 -*-
"""Modeling_DietCare.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x22tsdJaJ6S1Rf1R-Z481gEDlV4JQWpE

# **Modeling DietCare for Recommendation Recipe**

## **Download Dataset**
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d irkaal/foodcom-recipes-and-reviews

!unzip /content/foodcom-recipes-and-reviews.zip

"""## **Import Library**"""

import pandas as pd
import numpy as  np
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

from sklearn.metrics.pairwise import cosine_similarity
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam

"""## **Import Dataset**"""

data = pd.read_csv('/content/recipes.csv')

data.head()

import re

columns_rip = ["RecipeIngredientParts"]

for column in columns_rip:
    data[column] = data[column].apply(lambda x: re.sub(r'c\((.*?)\)', r'\1', x))

columns_to_modify = ["Images", "RecipeIngredientQuantities", "RecipeInstructions"]

for column in columns_to_modify:
    data[column] = data[column].astype(str).apply(lambda x: re.sub(r'c\("(.*?)"\)', r'\1', x))
    data[column] = data[column].astype(str).apply(lambda x: re.sub(r'c\(""\)', '', x))
    data[column] = data[column].astype(str).apply(lambda x: re.sub(r'c\(', '', x))
    data[column] = data[column].astype(str).apply(lambda x: re.sub(r'""', '', x))

data.head()

"""## **Data Preprocessing**"""

data.describe()

dataset=data.copy()
columns=['Name', 'Images', 'CookTime',
         'PrepTime', 'TotalTime', 'RecipeIngredientParts',
         'Calories', 'FatContent', 'SaturatedFatContent',
         'CholesterolContent', 'SodiumContent', 'CarbohydrateContent',
         'FiberContent','SugarContent','ProteinContent',
         'RecipeServings', 'RecipeInstructions'
         ]
dataset=dataset[columns]

dataset.head()

max_Calories=2650
max_daily_fat=100
max_daily_Saturatedfat=13
max_daily_Cholesterol=300
max_daily_Sodium=2300
max_daily_Carbohydrate=325
max_daily_Fiber=40
max_daily_Sugar=40
max_daily_Protein=200
max_list=[max_Calories,
          max_daily_fat,
          max_daily_Saturatedfat,
          max_daily_Cholesterol,
          max_daily_Sodium,
          max_daily_Carbohydrate,
          max_daily_Fiber,
          max_daily_Sugar,
          max_daily_Protein]

extracted_data=dataset.copy()
for column,maximum in zip(extracted_data.columns[6:15],max_list):
    extracted_data=extracted_data[extracted_data[column]<maximum]

extracted_data.shape

extracted_data = extracted_data.dropna(axis=0)

extracted_data.shape

extracted_data.describe()

extracted_data = extracted_data[extracted_data.Images != 'character(0)']

extracted_data.head()

extracted_data.to_csv('recipesprepross.csv', index=False)

"""## **Modeling**

### **System Recommendation based on Nutrition**
"""

# Select relevant columns for nutritional values
nutritional_columns = ['Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',
                       'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent']
recipe_data = extracted_data[nutritional_columns]

# Preprocess the dataset
recipe_data = recipe_data.fillna(0)  # Replace missing values with 0 or appropriate values
scaler = MinMaxScaler()
recipe_data_scaled = scaler.fit_transform(recipe_data)  # Scale the features to [0, 1]

# Split the dataset into training and validation sets
train_data, val_data = train_test_split(recipe_data_scaled, test_size=0.3, random_state=42)

# Build the recommendation model
input_dim = recipe_data_scaled.shape[1]

model = Sequential()
model.add(Dense(32, activation='relu', input_dim=input_dim))
model.add(Dense(16, activation='relu'))
model.add(Dense(input_dim, activation='linear'))

model.compile(loss='mean_squared_error', optimizer=Adam())

# Train the model
history = model.fit(train_data, train_data, epochs=20, batch_size=32, validation_data=(val_data, val_data))

def plot_loss(history):
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_loss(history)

# Generate recommendations for a given recipe
input_recipe = np.array([[228.0, 7.1, 1.7, 24.5, 281.8, 37.5, 0.5, 24.7, 4.2]])

model.predict(input_recipe)

# Generate recommendations for a given recipe
input_recipe = np.array([[206.42883 ,  25.310001,  24.869612,  53.37231 , 253.00551 , 47.74173 ,  -2.258176,  14.944985,  -4.963358]])  # Replace with your input recipe

# Scale the input recipe using the same scaler
input_recipe_scaled = scaler.transform(input_recipe)

# Calculate similarity scores
similarity_scores = cosine_similarity(input_recipe_scaled, recipe_data_scaled)

# Get recommended recipe indices
top_indices = np.argsort(similarity_scores, axis=1)[0][::-1][:10]  # Get top 10 indices

# Filter out irrelevant columns
relevant_columns = ['Name', 'Images', 'CookTime', 'PrepTime', 'TotalTime', 'RecipeIngredientParts',
                    'Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',
                    'SodiumContent', 'CarbohydrateContent', 'FiberContent',
                    'SugarContent', 'ProteinContent', 'RecipeServings', 'RecipeInstructions']
recommendations = extracted_data.iloc[top_indices][relevant_columns]

recommendations.head()

recjs = recommendations.to_dict("records")
print(recjs)

"""### **System Recommendation based on Calories**"""

# Preprocess the data
calories = extracted_data['Calories'].values.reshape(-1, 1)
mean_calories = np.mean(calories)
std_calories = np.std(calories)
normalized_calories = (calories - mean_calories) / std_calories

# Create the TensorFlow model
input_dim_2 = normalized_calories.shape[1]

model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_dim=input_dim_2),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Compile the model
model_2.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history_2 = model_2.fit(normalized_calories, normalized_calories, epochs=20, batch_size=32, validation_split=0.2)

plot_loss(history_2)

from sklearn.metrics.pairwise import euclidean_distances

# Define the recommendation function
def get_recommendations(input_calories, top_k=5):
    # Calculate the Euclidean distances between input calories and all recipes
    distances = euclidean_distances(extracted_data[['Calories']], [[input_calories]])

    # Add distances as a new column in the dataset
    extracted_data['Distance'] = distances

    # Sort recipes based on the distances in ascending order
    sorted_recipes = extracted_data.sort_values('Distance')

    # Select top k recipes
    top_recipes = sorted_recipes.head(top_k)

    return top_recipes

# Test the recommendation system
input_calories = 1000  # Example input calories
recommended_recipes = get_recommendations(input_calories, top_k=5)

recommended_recipes.head()

input_cal = [1000]

modelmadul = model_2.predict(input_cal)

print(modelmadul)

"""## **Convert Model for Deployment**"""

model.save("model.h5")

model_2.save("model_2.h5")



calories = 1000
prediction = model_2.predict([calories])

# Calculate the Euclidean distances between input calories and all recipes
distances = euclidean_distances(extracted_data[['Calories']], prediction)

# Add distances as a new column in the extracted_data
extracted_data['Distance'] = distances

# Sort recipes based on the distances in ascending order
sorted_recipes = extracted_data.sort_values('Distance')

# Select top k recipes
top_recipes = sorted_recipes.head(5)

# Test the recommendation system
# recommended_recipes = top_recipes(prediction, top_k=5)

trjs = top_recipes.to_dict('records')

print(trjs)